# 爬虫

## curl的使用

注意：安装的时候可能会遇到报错，有可能是openssl没装，

```bash
apt install curl
apt install openssl
apt install openssl-dev
```

一些常用参数的用法

| 参数 | 说明                                | 示例                                                         |
| ---- | ----------------------------------- | ------------------------------------------------------------ |
| -A   | 设置user-agent                      | curl -A "Chrome" http://www.baidu.com                        |
| -X   | 用指定方法请求                      | curl -X POST http://httpbin.org/post                         |
| -I   | 只返回请求的头信息                  |                                                              |
| -d   | 以POST方法请求url，并发送相应的参数 | -d a=1 -d b=2 -d c=3<br />-d "a=1&b=2&c=3"<br />-d @filename |
| -O   | 下载文件并以远程的文件名保存        |                                                              |
| -o   | 下载文件并以指定的文件名保存        | curl -o fox.jpeg http://httpbin.org/image/jpeg               |
| -L   | 跟随重定向请求                      | curl -IL https://baidu.com                                   |
| -H   | 设置头信息                          | curl -o image.webp -H "accept:image/webp" http://httpbin.org/image |
| -k   | 允许发起不安全的SSL请求             |                                                              |
| -b   | 设置cookies                         | curl -b a=test http://httpbin.org/cookies                    |
| -s   | 不显示其他无关信息                  |                                                              |
| -v   | 显示连接过程中的所有信息            |                                                              |

自定义一个命令，查看本机外网IP

```
alias myip="curl http://httpbin.org/get|grep -E '\d+'|grep -v User-Agent|cut -d '\"' -f4"
```

补充：

```
cut：
-b：仅显示行中指定直接范围的内容；
-c：仅显示行中指定范围的字符；
-d：指定字段的分隔符，默认的字段分隔符为“TAB”；
-f：显示指定字段的内容；
-n：与“-b”选项连用，不分割多字节字符；
--complement：补足被选择的字节、字符或字段；
--out-delimiter=<字段分隔符>：指定输出内容是的字段分割符；
--help：显示指令的帮助信息；
--version：显示指令的版本信息。
```

grep：

```
-a   --text   #不要忽略二进制的数据。   

-A<显示行数>   --after-context=<显示行数>   #除了显示符合范本样式的那一列之外，并显示该行之后的内容。   

-b   --byte-offset   #在显示符合样式的那一行之前，标示出该行第一个字符的编号。   

-B<显示行数>   --before-context=<显示行数>   #除了显示符合样式的那一行之外，并显示该行之前的内容。   

-c    --count   #计算符合样式的列数。   

-C<显示行数>    --context=<显示行数>或-<显示行数>   #除了显示符合样式的那一行之外，并显示该行之前后的内容。   

-d <动作>      --directories=<动作>   #当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。   

-e<范本样式>  --regexp=<范本样式>   #指定字符串做为查找文件内容的样式。   

-E      --extended-regexp   #将样式为延伸的普通表示法来使用。   

-f<规则文件>  --file=<规则文件>   #指定规则文件，其内容含有一个或多个规则样式，让grep查找符合规则条件的文件内容，格式为每行一个规则样式。   

-F   --fixed-regexp   #将样式视为固定字符串的列表。   

-G   --basic-regexp   #将样式视为普通的表示法来使用。   

-h   --no-filename   #在显示符合样式的那一行之前，不标示该行所属的文件名称。   

-H   --with-filename   #在显示符合样式的那一行之前，表示该行所属的文件名称。   

-i    --ignore-case   #忽略字符大小写的差别。   

-l    --file-with-matches   #列出文件内容符合指定的样式的文件名称。   

-L   --files-without-match   #列出文件内容不符合指定的样式的文件名称。   

-n   --line-number   #在显示符合样式的那一行之前，标示出该行的列数编号。   

-q   --quiet或--silent   #不显示任何信息。   

-r   --recursive   #此参数的效果和指定“-d recurse”参数相同。   

-s   --no-messages   #不显示错误信息。   

-v   --revert-match   #显示不包含匹配文本的所有行。   

-V   --version   #显示版本信息。   

-w   --word-regexp   #只显示全字符合的列。   

-x    --line-regexp   #只显示全列符合的列。   

-y   #此参数的效果和指定“-i”参数相同。 
```



## wget

安装：

```apt install wget```

参数说明

| A            | B                            | C                                             |
| ------------ | ---------------------------- | --------------------------------------------- |
| -O           | 以指定文件名保存下载的文件   | wget -O test.png http://httpbin.org/image/png |
| --limit-rate | 以指定的速度下载目标文件     | --limit-rate=200k                             |
| -c           | 断点续传                     |                                               |
| -b           | 后台下载                     |                                               |
| -U           | 设置User-Agent               |                                               |
| --mirror     | 镜像某个目标网站             |                                               |
| -p           | 下载页面中的所有相关资源     |                                               |
| -r           | 递归下载所有网页中所有的链接 |                                               |

```bash
# 镜像下载整个网站并保存到本地
wget -c --mirror -U "Mozilla" -p --convert-links http://docs.python-requests.org
--convert-links相当于 -r 递归下载所有链接
```

## httpie

```bash
apt install httpie
pip install httpie
```

## 代理服务器搭建

安装： sudo apt install tinyproxy

修改：sudo vim /etc/tinproxy.conf

- 把allow一行注释
- 把端口改为41801

重启：systemctl restart tinproxy

设置每过5分钟自动重启：

linux命令： 文件中按u 回退 

crontab -e

每五分钟执行： */5 * * * * systemctl restart tinproxy

每小时执行：0 * * * *

每天执行： 0 0 * * *

每周执行：0 0 * * 0

每月执行：0 0 1 * *

每年执行：0  0  1  1  *



## python中自带服务器

进入工程目录
python3 -m http.server



## python中redis操作

### redis基本命令 set

```
1.新增
sadd(name,values)
name对应的集合中添加元素

2.获取元素个数 类似于len
scard(name)
获取name对应的集合中元素个数

3.获取集合中所有的成员
smembers(name)
获取name对应的集合的所有成员

获取集合中所有的成员--元组形式
sscan(name, cursor=0, match=None, count=None)

获取集合中所有的成员--迭代器的方式
sscan_iter(name, match=None, count=None)
同字符串的操作，用于增量迭代分批获取元素，避免内存消耗太大

4.差集
sdiff(keys, *args)
在第一个name对应的集合中且不在其他name对应的集合的元素集合

5.差集--差集存在一个新的集合中
sdiffstore(dest, keys, *args)
获取第一个name对应的集合中且不在其他name对应的集合，再将其新加入到dest对应的集合中

6.交集
sinter(keys, *args)
获取多一个name对应集合的交集

7.交集--交集存在一个新的集合中
sinterstore(dest, keys, *args)
获取多一个name对应集合的并集，再将其加入到dest对应的集合中

并集
sunion(keys, *args)
获取多个name对应的集合的并集

并集--并集存在一个新的集合
sunionstore(dest,keys, *args)
获取多一个name对应的集合的并集，并将结果保存到dest对应的集合中

8.判断是否是集合的成员 类似in
sismember(name, value)
检查value是否是name对应的集合的成员，结果为True和False

9.移动
smove(src, dst, value)
将某个成员从一个集合中移动到另外一个集合

10.删除--随机删除并且返回被删除值
spop(name)
从集合移除一个成员，并将其返回,说明一下，集合是无序的，所有是随机删除的

11.删除--指定值删除
srem(name, values)
在name对应的集合中删除某些值
```

## 一行命令实现图片的下载

```
curl -s http://www.xiachufang.com/|grep -oP '(?<=src=\")http://i2\.chuimg\.com/\w+\.jpg'|xargs -i curl --create-dir {} -o ./images/{}

说明：
?<=字符串1：表示只匹配字符串1后面的内容

?= 字符串2：表示只匹配字符串2前面的内容
-o：精确匹配
-P：（Perl引擎匹配）更多元化匹配

xargs：多行输入单行输出
-i：可将值赋给{}
curl --create-file：创建必要的目录
```

## urllib使用

```
import urllib.request
import json


# 接收一个字符串作为参数
r = urllib.request.urlopen('http://httpbin.org/get')
# 读取response的内容
text = r.read()
print(text)
# http返回状态码和msg
print(r.status, r.reason)
r.close()

# 返回的内容是json格式，直接用Load函数加载
obj = json.loads(text)
print(obj)

# r.headers是一个HTTPMessage对象
# print(r.headers)
for k, v in r.headers._headers:
    print('%s: %s' % (k, v))

ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) ' \
     'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77' \
     ' Safari/537.36'
# 添加自定义的头信息
req = urllib.request.Request('http://httpbin.org/user-agent')
req.add_header('User-Agent', ua)
# 接收一个urllib.request.Request对象作为参数
r = urllib.request.urlopen(req)
resp = json.load(r)
# 打印出httpbin网站返回信息里的user-agent
print("user-agent: ", resp["user-agent"])


# 发起带basic auth的请求
auth_handler = urllib.request.HTTPBasicAuthHandler()
auth_handler.add_password(realm='Fake Realm',
                          uri='http://httpbin.org',
                          user='guye',
                          passwd='123456')
opener = urllib.request.build_opener(auth_handler)
urllib.request.install_opener(opener)
r = urllib.request.urlopen('http://httpbin.org/basic-auth/guye/123456')
print(r.read().decode('utf-8'))

# 使用GET参数
params = urllib.parse.urlencode({'spam': 1, 'eggs':2, 'bacon': 2})
url = 'http://httpbin.org/get?%s' % params
with urllib.request.urlopen(url) as f:
    print(json.load(f))

# 使用POST方法传递参数
data = urllib.parse.urlencode({'name': '小明', 'age': 2})
data = data.encode()
with urllib.request.urlopen('http://httpbin.org/post', data) as f:
    print(json.load(f))

# 使用代理IP请求远程url
proxy_handler = urllib.request.ProxyHandler({
                        'http': 'http://iguye.com:41801'
                    })
# proxy_auth_handler = urllib.request.ProxyBasicAuthHandler()
opener = urllib.request.build_opener(proxy_handler)
r = opener.open('http://httpbin.org/ip')
print(r.read())


# urlparse模块
o = urllib.parse.urlparse('http://httpbin.org/get')
```

## requests使用

```
import requests

# GET请求
r = requests.get('http://httpbin.org/get')
print(r.status_code, r.reason)
print('GET请求', r.text)
# 带参数的GET请求
r = requests.get('http://httpbin.org/get', params={'a': '1', 'b': '2'})
print('带参数的GET请求', r.json())

#POST请求
r = requests.post('http://httpbin.org/post', data={'a': '1'})
print('POST请求', r.json())

# 自定义headers请求
ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) ' \
     'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77' \
     ' Safari/537.36'
headers = {'User-Agent': ua}
r = requests.get('http://httpbin.org/headers', headers=headers)
print('自定义headers请求', r.json())

# 带cookies的请求
cookies = dict(userid='123456', token='xxxxxxxxxxxxxxxxxxxx')
r = requests.get('http://httpbin.org/cookies', cookies=cookies)
print('带cookies的请求', r.json())

# Basic-auth认证请求
r = requests.get('http://httpbin.org/basic-auth/guye/123456', auth=('guye', '123456'))
print('Basic-auth认证请求', r.json())

# 主动抛出状态码异常
bad_r = requests.get('http://httpbin.org/status/404')
print(bad_r.status_code)
bad_r.raise_for_status()

#使用requests.Session对象请求
# 创建一个Session对象
s = requests.Session()
# session对象会保存服务器返回的set-cookies头信息里面的内容
s.get('http://httpbin.org/cookies/set/userid/123456789')
s.get('http://httpbin.org/cookies/set/token/xxxxxxxxxxxxxxxxxx')
# 下一次请求会将本地所有的cookies信息自动添加到请求头信息里面
r = s.get('http://httpbin.org/cookies')
print('检查session中的cookies', r.json())


# 在requests中使用代理
print('不使用代理：', requests.get('http://httpbin.org/ip').json())
print('使用代理：', requests.get(
    'http://httpbin.org/ip',
    proxies={'http': 'http://iguye.com:41801'}
).json())

r = requests.get('http://httpbin.org/delay/4', timeout=5)
print(r.text)
```

## pycurl使用

```
import re
import os
from io import BytesIO
from urllib.parse import urlparse
from pycurl import Curl


buffer = BytesIO()
c = Curl()
c.setopt(c.URL, 'http://www.xiachufang.com/')
c.setopt(c.WRITEDATA, buffer)
c.perform()
c.close()
body = buffer.getvalue()
text = body.decode('utf-8')
print(text)


img_list = re.findall(r'src=\"(http://i2\.chuimg\.com/\w+\.jpg)',
                      text)

# 初始化下载文件目录
image_dir = os.path.join(os.curdir, 'images')
# if not os.path.isdir(image_dir):
#     os.mkdir(image_dir)

for img in img_list[::-1]:
    o = urlparse(img)
    filename = o.path[1:]
    filepath = os.path.join(image_dir, filename)
    if not os.path.isdir(os.path.dirname(filepath)):
        os.mkdir(os.path.dirname(filepath))
    url = '%s://%s/%s' % (o.scheme, o.netloc, filename)
    print(url)
    with open(filepath, 'wb') as f:
        c = Curl()
        c.setopt(c.URL, url)
        c.setopt(c.WRITEDATA, f)
        c.perform()
        c.close()
```

## bs4使用

```
from bs4 import BeautifulSoup

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""
# 创建一个BeautifulSoup对象
soup = BeautifulSoup(html_doc)
print(soup.prettify())
soup.title
type(soup.title)
dir(soup.title)
soup.title.text
#取出第一个a标签的所有属性
soup.a.attrs
# 取出a标签的href属性
soup.a.attrs['href']
#判断是否有class属性
soup.a.has_attr('class')
#取出第一个p标签下的所有的子节点
soup.p.children
# 取出来的结果是一个迭代器，所以用list转换一下
list(soup.p.children)
list(soup.p.children)[0]
list(soup.p.children)[0].text

# 取出本页面内所有的链接
for a in soup.find_all('a'):
    print(a.attrs['href'])

# 取出id=link3的那个节点
soup.find(id='link3')
# 取出页面内所有的文本
soup.get_text()

# 支持CSS选择器
# 查找类名为story的节点
soup.select('.story')
# 查找id=link1的节点
soup.select('#link1')


soup_lxml = BeautifulSoup(html_doc, 'lxml')
print('##' * 10, soup_lxml.a)
```

## lxml使用

```
from lxml import etree
import requests


html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""


selector = etree.HTML(html_doc)
# 取出页面内所有的链接
links = selector.xpath('//p[@class="story"]/a/@href')
for link in links:
    print(link)


r = requests.get('http://iguye.com/books.xml')
se = etree.HTML(r.text)
print(se.xpath('book'))

print(se.xpath('//book'))
# 选取书店下所有的书本的作者的名字
print(se.xpath('//bookstore/book/author/text()'))
# 选取书店下所有的书本的语言
print(se.xpath('//bookstore/book/title/@lang'))

# 选取书店下第一本书的标题
print(se.xpath('//bookstore/book[1]/title/text()'))
# 选取书店下最后一本书的标题
print(se.xpath('//bookstore/book[last()]/title/text()'))
# 选取书店下倒数第二本书的标题
print(se.xpath('//bookstore/book[last()-1]/title/text()'))
# 选取书店下前2本书的标题
print('前2本书',
      se.xpath('//bookstore/book[position()<3]/title/text()'))
# 选取所有的分类为web的书本
print('分类为web的书本', se.xpath('//book[@category="web"]/title/text()'))

# 选取所有价格大于30.00元的书本
print('价格大于30.00元的书本', se.xpath('//book[price>35.00]/price/text()'))

# 选取所有class属性中包含book的书本的class属性
print('类名中包含book的书本', se.xpath('//book[contains(@class, "book")]/@class'))
```
# scrapy框架

## scrapy的安装

scrapy的底层依赖于lxml, twisted, openssl，涉及到系统C库，所以有可能会导致安装失败。

```
pip install scrapy
apt install python3-scrapy
```

## scrapy命令

### 创建项目

```
scrapy startproject qianmu
```

### 生成spider文件

注意：爬虫名字不要和项目名字重复

```bash
#scrapy genspider [爬虫名字] [目标网站域名]
scrapy genspider usnews qianmu.iguye.com
```

### 运行爬虫

```bash
# 运行名为usnewz的爬虫
scrapy crawl usnews
# 将爬到的数据导出为Json文件
scrapy crawl usnews -o usnews.json
# 导出为csv文件
scrapy crawl usnews -o usnews.csv -t csv
# 单独运行爬虫文件
scrapy runspider usnews.py
```

### 调试爬虫

```bash
# 进入到scrapy控制台，使用的是项目的环境
scrapy shell
# 带一个URL参数，将会自动请求这个url，并在请求成功后进入控制台
scrapy shell http://url.com

#类似断点调试：
from scrapy.shell import inspect_response
inspect_response(response,self)

```

进入到控制台以后，可以使用以下函数和对象

| A        | B                                                            |
| -------- | ------------------------------------------------------------ |
| fetch    | 请求url或者Requesrt对象，注意：请求成功以后会自动将当前作用域内的request和responsne对象重新赋值 |
| view     | 用浏览器打开response对象内的网页                             |
| shelp    | 打印帮助信息                                                 |
| spider   | 相应的Spider类的实例                                         |
| settings | 保存所有配置信息的Settings对象                               |
| crawler  | 当前Crawler对象                                              |
| scrapy   | scrapy模块                                                   |



```bash
# 用项目配置下载网页，然后用浏览器打开网页
scrapy view url
# 用项目配置下载网页，然后输出至控制台
scrapy fetch url
```



## 中间件

### process_request

在request对象传往downloader的过程中调用。当返回不同类型的值的时候，行为也不一样：

| 返回值        | 行为                                                         |
| ------------- | ------------------------------------------------------------ |
| None          | 一切正常，继续执行其他的中间件链                             |
| Response      | 停止调用其他process_request和process_exception函数，也不再继续下载该请求，然后走调用process_response的流程 |
| Request       | 不再继续调用其他process_request函数，交由调度器重新安排下载。 |
| IgnoreRequest | process_exception函数会被调用，如果没有此方法，则request.errback会被调用，如果errback也没有，则此异常会被忽略，甚至连日志都没有。 |

### process_response

在将下载结果返回给engine过程中被调用

| 返回值        | 行为                                                         |
| ------------- | ------------------------------------------------------------ |
| Response      | 续续调用其他中间件的process_response                         |
| Request       | 不再继续调用其他process_request函数，交由调度器重新安排下载。 |
| IgnoreRequest | 则request.errback会被调用，如果errback也没有，则此异常会被忽略，甚至连日志都没有。 |

### process_exception

在下载过程中出现异常，或者在process_request中抛出IgnoreRequest异常的时候调用。

| 返回值   | 行为                                                         |
| -------- | ------------------------------------------------------------ |
| Response | 开始中间件链的process_response处理流程                       |
| Request  | 不再继续调用其他process_request函数，交由调度器重新安排下载。 |
| None     | 继续调用其他中间件里的process_exception函数                  |



### from_crawler(cls, crawler)

如果存在该函数，则调用该函数创建中间件的实例。如果要写这个函数，一定要返回一个中间件的对象。

### 内置中间件

请求robots.txt文件，并解析其中的规则。

scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware

执行带Basic-auth验证的请求

scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware

下载请求超时最大时长

scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware

设置默认的请求头信息

scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware

设置请求头信息里的User-Agent

scrapy.downloadermiddlewares.useragent.UserAgentMiddleware

如果下载失败，是否重试，重试几次

scrapy.downloadermiddlewares.retry.RetryMiddleware

实现Meta标签重定向

scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware

实现压缩内容的解析（比如gzip）

scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware

实现30x的HTTP code的重定向

scrapy.downloadermiddlewares.redirect.RedirectMiddleware

实现对cookies的设置管理

scrapy.downloadermiddlewares.cookies.CookiesMiddleware

实现IP代理

scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware

下载信息的统计

scrapy.downloadermiddlewares.stats.DownloaderStats

下载结果的缓存

scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware

例子：

```
添加代理，管理代理：
class RandomProxyMiddleware(object):

    def __init__(self, settings):
        # 2. 初始化配置及相关变量
        self.proxies = settings.getlist('PROXIES')
        self.stats = defaultdict(int)
        self.max_failed = 3

    @classmethod
    def from_crawler(cls, crawler):
        # 1. 创建中间件对象
        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):
            raise NotConfigured

        return cls(crawler.settings)

    def process_request(self, request, spider):
        # 3. 为每个request对象分配一个随机的IP代理
        if self.proxies and not request.meta.get('proxy') \
                and request.url not in spider.start_urls:
            request.meta['proxy'] = random.choice(self.proxies)

    def process_response(self, request, response, spider):
        # 4. 请求成功则调用process_response
        cur_proxy = request.meta.get('proxy')
        # 判断是否被对方封禁
        if response.status in (401, 403):
            # 给相应的IP失败次数+1
            self.stats[cur_proxy] += 1
            print('%s got wrong code %s times' % (cur_proxy, self.stats[cur_proxy]))
        # 当某个IP的失败次数累计到一定数量
        if self.stats[cur_proxy] >= self.max_failed:
            print('got wrong http code (%s) when use %s' \
                  % (response.status, cur_proxy))
            # 可以认为该IP被对方封禁了，从代理池中将该IP删除
            self.remove_proxy(cur_proxy)
            del request.meta['proxy']
            # 将该请求重新安排调度下载
            return request
        return response

    def process_exception(self, request, exception, spider):
        # 4. 请求失败则调用process_exception
        cur_proxy = request.meta.get('proxy')
        # 如果本次请求使用了代理，并且网络请求报错，认为该IP出现问题了
        if cur_proxy and isinstance(exception, (ConnectionRefusedError, TimeoutError)):
            print('error (%s) occur when use proxy %s' % (exception, cur_proxy))
            self.remove_proxy(cur_proxy)
            del request.meta['proxy']
            return request

    def remove_proxy(self, proxy):
        if proxy in self.proxies:
            self.proxies.remove(proxy)
            print('remove %s from proxy list' % proxy)
```



## 扩展

```
实现爬虫监控，实时报警：
import logging
from collections import defaultdict
from datetime import datetime
from scrapy import signals
from scrapy.exceptions import NotConfigured

logger = logging.getLogger(__name__)


class SpiderOpenCloseLogging(object):

    def __init__(self, item_count):
        self.item_count = item_count
        self.items_scraped = 0
        self.items_dropped = 0
        self.stats = defaultdict(int)
        self.error_stats = defaultdict(int)
        print("==" * 20, 'Extension object created')

    @classmethod
    def from_crawler(cls, crawler):
        # first check if the extension should be enabled and raise
        # NotConfigured otherwise
        if not crawler.settings.getbool('MYEXT_ENABLED'):
            raise NotConfigured

        # get the number of items from settings
        item_count = crawler.settings.getint('MYEXT_ITEMCOUNT', 1000)

        # instantiate the extension object
        ext = cls(item_count)

        # connect the extension object to signals
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)
        crawler.signals.connect(ext.item_dropped, signal=signals.item_dropped)
        crawler.signals.connect(ext.response_received, signal=signals.response_received)

        # return the extension object
        return ext

    def spider_opened(self, spider):
        print("==" * 20, "opened spider %s" % spider.name)

    def spider_closed(self, spider):
        print("==" * 20, "closed spider %s" % spider.name)

    def item_scraped(self, item, spider):
        self.items_scraped += 1
        if self.items_scraped % self.item_count == 0:
            print("=="* 20, "scraped %d items" % self.items_scraped)

    def item_dropped(self, item, spider, response, exception):
        self.items_dropped += 1
        if self.items_dropped % self.item_count == 0:
            print("**" * 20, "dropped %d items" % self.items_dropped)

    def response_received(self, response, request, spider):
        now = datetime.now().strftime('%Y%m%d%H%M')
        self.stats[now] += 1
        if response.status in [401, 403, 404, 500, 501, 502]:
            self.error_stats[now] += 1
        if self.error_stats[now] / float(self.stats[now]) > 0.2:
            logger.warning('received %s response'
                           ', and %s of them is none 200 in %s' % \
                           (self.stats[now], self.error_stats[now], now))
```

## 信号

## scrapy发送post请求

通过FormRequest类发送请求

request = FormRequest(formdata=data, url=url, callback=self.parse_link, headers=headers)



# 反爬虫经验

## 爬虫

新片场

- 动态加载网页
  - 抓取xhr（接口）
  - 分析js找出xhr请求连接规律
- 爬取20页以后需要登录
- 爬取100多页以后换cookie中的phpsessionid值
- 注意代理的随机更换和user-agent的随机性
- 坑点：
  - 注意scrapy框架allowed_domains限制爬取域名（不报错、不执行函数）

去哪儿网

- 使用selenium爬取：
- 难点
  - 机票价格可能是通过js动态生成
  - 规律：
    - 结合css中样式width和left属性计算出要替换的位置，然后用当前位置的值去替换，这样才是真实的价格

总结：

请求头信息：

- referer，content-type，请求方法（post、get）
- 构造请求头

异步加载

- 我们需要分析页面的网络请求，从中找出和我们想要的数据相关的请求，并分析他的请求头信息、参数、cookie，然后根据这些信息构造我们的请求。通常说的是ajax请求，也有图片请求，比如图片的lazy load，通过js在页面加载后修改图片的src属性一般都会有其他的自定义属性存在，比如说“”

大众点评：

svg图片文字，用span动态替换文字



# selnium

## 概念

selnium是一个自动化测试框架，我们主要用它的webdriver。



## webdriver

支持各大主流浏览器

### chromedriver安装:

安装chromedriver,下载压缩包，解压以后，把可执行文件所在的目录放到PATH环境变量里，比如，chromedriver在/home/user/

```bash
export PATH="$PATH:/home/user"
```

如果不设置环境变量，则需要在启动的时候指定chromedriver的路径。



### 启动浏览器

```python
from selenium import webdriver
# 启动chrome浏览器
driver = webdriver.Chrome()
# 指定chromedriver的路径并启动Chrome
driver = webdriver.Chrome(executable_path='/home/user/chromedirver')
#启动chrome-headless
from selenium.webdriver.chrome.options import Options
option = Options()
option.add_argument('--headless')
driver = webdriver.Chrome(chrome_options=option)

# 启动phantomjs
driver = webdriver.PhantomJs()
```

chrome-headless是无界面版的chrome，它替代了停止维护的phantomjs.

### 控制浏览器

```python
# 访问某个url
driver.get('https://www.baidu.com')
# 刷新
driver.refersh()
# 前进
driver.forward()
# 后退
driver.back()
#退出
driver.quit()
# 当前的url
driver.current_url
# 截图
driver.save_screenshot('/tmp/test.png')
```



### 元素查找

18个find函数

```python
# 根据元素的class属性的值查找 
driver.find_element_by_class_name
# 用CSS选择器查找
driver.find_element_by_css_selector
# 根据元素的ID
 driver.find_element_by_id
# 根据链接内的文本查找
find_element_by_link_text
# 根据元素的name属性查找 
find_element_by_name
# 根据链接内的文本是否包含指定的查找文字
find_element_by_partial_link_text
# 根据标签名查找
find_element_by_tag_name
# 根据xpath表达示查找 
find_element_by_xpath
```

另外，以上函数分别还有个查找多个元素的对应函数，把element变成复数，比如: find_elements_by_xpath,
注意：find_element函数返回单个WebElement对象，
而所有的find_elements则返回一个列表，包含若干个WebElement对象
如果查找不到，则抛出异常



### page_source

获取整个页面的源码

### get_cookie/get_cookies

获取服务器返回的cookie



### 执行javascript

```python
# 将网页滚动到最底部
driver.execute('window.scrollTo(0, document.body.scrollHeight)')
# 执行异步的js函数
driver.execute_async_script('send_xml_request()')
```



### 等待,wait

#### 硬性等待

```python
#使用time.sleep(3)硬性等待
```



#### 隐式等待

```python
# 查找某个(某些)元素，如果没有立即查找到，则等待10秒
driver.implicityly_wait(10)
```

#### 显式等待

```python
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
# 查找一个按钮
# 最长等待10秒，直到找到查找条件中指定的元素
sort_btn = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, './/div[@class="f-sort"]/a[2]'))
    )

```

这两个等待，显示等待通常更符合我们的程序逻辑。当我们对页面的加载方式还不太确定的时候，也可以隐式等待。

## WebElement



也有一堆的find函数，用法和之前一样。

### click

点击某个具体的元素

### send_keys

向某个元素（通常是可输入的标签，比如input）发送键盘事件

```python
# 向input元素内输入iphone
input.send_keys('iphone')
# 向input发送一个回车键
from selenium.webdriver.common.keys import Keys
input.send_keys(Keys.RETURN)
```

### rect

返回元素的宽和高，以及在屏幕上的坐标

```python
# 返回结果
{u'height': 24, u'width': 26, u'x': 347.1875, u'y': 19}
```

### location

返回在屏幕上的坐标

### text

返回元素内的文本信息、

注意：我们在selenium内查找元素时，不可以直接查找它的文本，比如写xpath表达式的时候， 不可以用text()函数，要先取到节点，然后再用它的text属性取出文本。



get_attribute

get_proerty

获取元素的属性值，同上，它也不可以在选择器里直接写，需要先取到节点，然后再调用这俩方法去获取属性值。

value_of_css_property

获取节点的CSS样式属性

